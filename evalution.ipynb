{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MultiNERD NER model evalution\n","Evaluating Transformer-based NER model for MultiNERD dataset downloaded from  ðŸ¤— Hub."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install huggingface_hub==0.19.4\n","# !pip install datasets==2.15.0\n","# !pip install seqeval==1.2.2\n","# !pip install evaluate==0.4.1\n","# !pip install torch==2.0.0\n","# !pip install transformers[torch]==4.35.2\n","# !pip install tqdm==4.66.1\n","\n","import json\n","import os\n","\n","from tqdm import tqdm\n","from datasets import load_dataset\n","import evaluate\n","from transformers import RobertaTokenizerFast\n","from transformers import AutoModelForTokenClassification\n","import torch\n","os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset = load_dataset(\"Babelscape/multinerd\",split='test')\n","test_dataset = test_dataset.filter(lambda x: x['lang'] == 'en')\n","print(f\"Total Sequences in EN test dataset: {len(test_dataset)}\")\n","\n","with open('./config.json', 'r') as f:\n","    config = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["is_systemB = True #False for system A. True for system B.\n","\n","if is_systemB:\n","    model_checkpoint = \"./roberta-base-finetuned-ner-B/checkpoint-4000\"\n","    tokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True)\n","else:    \n","    model_checkpoint = \"./roberta-base-finetuned-ner-A/checkpoint-4000\"\n","    tokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint,add_prefix_space=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def system_labels(is_systemB, examples):\n","\n","  if is_systemB:\n","\n","    systemB_ids = [int(k) for k in config['systemBid'].keys()]\n","    ner_tags = []\n","    for labels in examples['ner_tags']:\n","      ner_tags.append([ label if label in systemB_ids else 0 for label in labels])\n","\n","    examples['ner_tags'] = ner_tags\n","\n","  return examples['ner_tags']\n","\n","\n","def tokenize_and_align_labels(examples, label_all_tokens = False):\n","    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    ner_tags = system_labels(is_systemB, examples)\n","    for i, label in enumerate(ner_tags):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs['labels'] = labels\n","    return tokenized_inputs\n"]},{"cell_type":"markdown","metadata":{},"source":["## Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, device_map=\"auto\")\n","\n","tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n","\n","metric = evaluate.load(\"seqeval\")\n","predictions = []\n","references = []\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","for tokens, labels in tqdm(zip(test_dataset['tokens'], tokenized_test_dataset['labels']), total= len(test_dataset['tokens'])):\n","\n","    tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True, return_tensors=\"pt\")\n","    tokenized_inputs = tokenized_inputs.to(device)\n","    with torch.no_grad():\n","        logits = model(**tokenized_inputs).logits\n","        \n","    prediction = torch.argmax(logits, dim=2)\n","    prediction = [model.config.id2label[t.item()] for t in prediction[0]]\n","    predictions.append(prediction)\n","    references.append(labels) \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Remove ignored index (special tokens)\n","true_predictions = [\n","    [p for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, references)\n","]\n","true_references = [\n","    [model.config.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, references)\n","]\n","\n","metric.add_batch(references=true_references, predictions=true_predictions)\n","metric.compute(mode='strict', scheme='IOB2',zero_division=0)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4082435,"sourceId":7132075,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
